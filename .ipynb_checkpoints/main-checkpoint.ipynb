{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7737a0f7",
   "metadata": {},
   "source": [
    "## Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0718db48",
   "metadata": {},
   "source": [
    "- Make a graph model of the flights going out of Wuhan in the beginning of the pandemic.\n",
    "- Before and after quarantine\n",
    "- Using centrality measures to find out which airports are the most important for the spread from China to other countries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f3d2cf",
   "metadata": {},
   "source": [
    "## Research question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5c63d2",
   "metadata": {},
   "source": [
    "Provide insight on the evolution of covid 19 in the USA. Note that providing insight is much more that producing a couple of nice charts. We expect to see a data scientist at work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945e8e2e",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ad136d",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cca5278-6b67-4e5b-bd31-6271b667a692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb613bd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data storage and manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from py2neo import Graph, Node, Relationship\n",
    "from py2neo.bulk import merge_nodes, merge_relationships, create_relationships\n",
    "import re\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objects as go\n",
    "import pyLDAvis\n",
    "\n",
    "# Extra information\n",
    "# from traffic.data import airports\n",
    "# airports_df = airports.data\n",
    "\n",
    "# Text mining\n",
    "# import gensim\n",
    "\n",
    "# Utility\n",
    "from tqdm.notebook import tqdm\n",
    "from urllib.request import urlopen\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b359cc3e",
   "metadata": {},
   "source": [
    "#### Checking the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2802dc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda info | grep 'active env'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9115aabd",
   "metadata": {},
   "source": [
    "#### Setting some options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc226757",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 1000)\n",
    "pd.set_option(\"display.float_format\", '{:.2f}'.format)\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats = ['svg']\n",
    "plotly.offline.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d27d89",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26851e09",
   "metadata": {},
   "source": [
    "#### Loading the COVID information that was provided with the assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d388c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/nytimes/covid-19-data\n",
    "covid_data_path = \"data/covid/\"\n",
    "\n",
    "def load_counties():\n",
    "    counties = pd.read_csv(covid_data_path + \"us-counties.csv\")\n",
    "    counties = counties.rename({\"cases\": \"cases_cum\"}, axis=1)\n",
    "    counties[\"date\"] = pd.to_datetime(counties[\"date\"])\n",
    "    counties[\"date_str\"] = counties[\"date\"].dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    def reverse_cumsum(series):\n",
    "        series_zeroed = pd.concat([pd.Series([0]), series])\n",
    "        return series_zeroed.diff()[1:]\n",
    "    \n",
    "    def undo_cases_cum(df):\n",
    "        df_edited = pd.DataFrame()\n",
    "        for code in df[\"fips\"].unique():\n",
    "            data = df.loc[df[\"fips\"] == code]\n",
    "            data[\"cases\"] = reverse_cumsum(data[\"cases_cum\"])\n",
    "            df_edited = pd.concat([df_edited, data])\n",
    "            \n",
    "        return df_edited\n",
    "    \n",
    "    counties = undo_cases_cum(counties)\n",
    "    \n",
    "    return counties\n",
    "\n",
    "\n",
    "counties = load_counties()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75930fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_states():\n",
    "    states = pd.read_csv(covid_data_path + \"us-states.csv\")\n",
    "    states[\"date\"] = pd.to_datetime(states[\"date\"])\n",
    "\n",
    "    return states\n",
    "\n",
    "\n",
    "states = load_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a68aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_us():\n",
    "    us = pd.read_csv(covid_data_path + \"us.csv\")\n",
    "    us = us.rename(columns={\"cases\": \"cases_cum\"})\n",
    "    us[\"date\"] = pd.to_datetime(us[\"date\"])\n",
    "    \n",
    "    def reverse_cumsum(series):\n",
    "        series_zeroed = pd.concat([pd.Series([0]), series])\n",
    "        return series_zeroed.diff()[1:]\n",
    "    \n",
    "    us[\"cases\"] = reverse_cumsum(us[\"cases_cum\"])\n",
    "    \n",
    "    return us\n",
    "\n",
    "\n",
    "us = load_us()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a71bbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prisons():\n",
    "    prisons = pd.read_csv(covid_data_path + \"prisons/facilities.csv\")\n",
    "    # Calculating how many cases there are per 100 inmates\n",
    "    prisons[\"cases_per_100\"] = prisons[\"total_inmate_cases\"] \\\n",
    "                               / (prisons[\"latest_inmate_population\"]*100)\n",
    "    # Omitting outliers\n",
    "    prisons = prisons[prisons[\"cases_per_100\"] < 2]\n",
    "    \n",
    "    # Categorizing the prison size\n",
    "    prisons[\"prison_size\"] = \"unknown\"\n",
    "    prisons.loc[prisons[\"latest_inmate_population\"].between(0, 1000),\n",
    "                \"prison_size\"] = \"small\"\n",
    "    prisons.loc[prisons[\"latest_inmate_population\"].between(1000, 3000),\n",
    "                \"prison_size\"] = \"medium\"\n",
    "    prisons.loc[prisons[\"latest_inmate_population\"].between(3000, np.inf),\n",
    "                \"prison_size\"] = \"large\"\n",
    "    \n",
    "    return prisons\n",
    "\n",
    "\n",
    "prisons = load_prisons()\n",
    "\n",
    "\n",
    "# # --- Loading information from colleges ---\n",
    "# colleges = pd.read_csv(covid_data_path + \"colleges/colleges.csv\")\n",
    "# \n",
    "# # --- Loading other information ---\n",
    "# deaths = pd.read_csv(covid_data_path + \"excess-deaths/deaths.csv\")\n",
    "# mask_use = pd.read_csv(covid_data_path + \"mask-use/mask-use-by-county.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19116969",
   "metadata": {},
   "source": [
    "#### Loading economical information in the USA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84343762",
   "metadata": {},
   "source": [
    "Economical information about each state was added to provide more robust explanations. The information was obtained from an American government agency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86df74c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://apps.bea.gov/regional/downloadzip.cfm\n",
    "# --- Loading economic information ---\n",
    "def load_gdp_data(path):\n",
    "    gdp_data_path = \"data/economical/SQGDP/\"\n",
    "    \n",
    "    df = pd.read_csv(gdp_data_path + path)\n",
    "    # Removing false observations\n",
    "    df = df.iloc[:-4]\n",
    "    # Transforming FIPS code to int\n",
    "    df[\"GeoFIPS\"] = df[\"GeoFIPS\"].str.replace('\"', \"\").astype(int)\n",
    "    # Stripping description from left and right spaces\n",
    "    df[\"Description\"] = df[\"Description\"].str.strip()\n",
    "    # Removing invalid data\n",
    "    df = df.replace(\"(D)\", np.nan)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def transform_gdp_data(data):\n",
    "    gdp_cols = ['2005:Q2', '2005:Q3', '2005:Q4', '2006:Q1', '2006:Q2',\n",
    "                '2006:Q3', '2006:Q4', '2007:Q1', '2007:Q2', '2007:Q3',\n",
    "                '2007:Q4', '2008:Q1', '2008:Q2', '2008:Q3', '2008:Q4',\n",
    "                '2009:Q1', '2009:Q2', '2009:Q3', '2009:Q4', '2010:Q1',\n",
    "                '2010:Q2', '2010:Q3', '2010:Q4', '2011:Q1', '2011:Q2',\n",
    "                '2011:Q3', '2011:Q4', '2012:Q1', '2012:Q2', '2012:Q3',\n",
    "                '2012:Q4', '2013:Q1', '2013:Q2', '2013:Q3', '2013:Q4',\n",
    "                '2014:Q1', '2014:Q2', '2014:Q3', '2014:Q4', '2015:Q1',\n",
    "                '2015:Q2', '2015:Q3', '2015:Q4', '2016:Q1', '2016:Q2',\n",
    "                '2016:Q3', '2016:Q4', '2017:Q1', '2017:Q2', '2017:Q3',\n",
    "                '2017:Q4', '2018:Q1', '2018:Q2', '2018:Q3', '2018:Q4',\n",
    "                '2019:Q1', '2019:Q2', '2019:Q3', '2019:Q4', '2020:Q1',\n",
    "                '2020:Q2', '2020:Q3', '2020:Q4', '2021:Q1', '2021:Q2',\n",
    "                '2021:Q3', '2021:Q4']\n",
    "\n",
    "    # It is generally not a good idea to loop over a dataframe, but I did not\n",
    "    # find another way to make this transformation\n",
    "    gdp_df = pd.DataFrame(index=gdp_cols)\n",
    "    for state in data[\"GeoName\"].unique():\n",
    "        data_state = data.loc[data[\"GeoName\"] == state]\n",
    "        for var in data_state[\"Description\"]:\n",
    "            gdp = data_state.loc[data[\"Description\"] == var][gdp_cols]\\\n",
    "                .transpose()\n",
    "            # 1d dataframe to series\n",
    "            gdp = gdp.iloc[:, 0]\n",
    "            # Renaming the series\n",
    "            gdp = gdp.rename(state + \"|\" + var)\n",
    "            # Adding to the data frame\n",
    "            gdp_df = gdp_df.join(gdp)\n",
    "\n",
    "    # Making the table long instead of wide\n",
    "    gdp_df = pd.melt(gdp_df, value_vars=gdp_df.columns.values,\n",
    "                     var_name=\"industry\", value_name=\"gdp\", ignore_index=False)\n",
    "\n",
    "    # Splitting industry and state\n",
    "    gdp_df[[\"state\", \"industry\"]] = gdp_df[\"industry\"].str\\\n",
    "        .split(\"|\", 1, expand=True)\n",
    "\n",
    "    # Including the index (quarter) as a variable\n",
    "    gdp_df[\"quarter\"] = gdp_df.index\n",
    "    gdp_df[\"quarter\"] = pd.to_datetime(gdp_df[\"quarter\"].str.replace(\":\", \"-\"))\n",
    "    \n",
    "    # Resetting the index\n",
    "    gdp_df = gdp_df.reset_index()\n",
    "    gdp_df = gdp_df.drop(\"index\", axis=1)\n",
    "    \n",
    "    # Changing column types\n",
    "    gdp_df[\"gdp\"] = pd.to_numeric(gdp_df[\"gdp\"])\n",
    "\n",
    "    return gdp_df\n",
    "\n",
    "\n",
    "\n",
    "gdp_state = load_gdp_data(\"SQGDP9__ALL_AREAS_2005_2021.csv\")\n",
    "gdp_state = transform_gdp_data(gdp_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0135f3",
   "metadata": {},
   "source": [
    "#### Loading flight information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5185a0d1",
   "metadata": {},
   "source": [
    "Thirdly, flight information was added to provide illustrations about the movement of persons during the start of the pandemic. The python package *traffic* was used to add information about the airports to this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990e359b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://zenodo.org/record/6411336#.YmO_gS8RrzU\n",
    "def load_flight_data(path, international=False):\n",
    "    \"\"\"Loads flight data. To save on memory, it is possible to discard non-\n",
    "       international flights. Merges information about airports.\"\"\"\n",
    "    flight_data_path = \"data/flights_open_sky/\"\n",
    "    df = pd.read_csv(flight_data_path + path)\n",
    "\n",
    "    # Dropping where important variables are null. We cannot do much with the\n",
    "    # data if the origin or destination of the flight are not known.\n",
    "    df = df.loc[-df[\"origin\"].isnull()]\n",
    "    df = df.loc[-df[\"destination\"].isnull()]\n",
    "\n",
    "    # Type changes\n",
    "    df[\"firstseen\"] =  pd.to_datetime(df[\"firstseen\"])\n",
    "    df[\"lastseen\"] =  pd.to_datetime(df[\"lastseen\"])\n",
    "    df[\"day\"] =  pd.to_datetime(df[\"day\"])\n",
    "\n",
    "    # Adding information about the origin and destination airport\n",
    "    df = df.merge(airports_df.add_prefix(\"origin_\"), how=\"left\",\n",
    "                  left_on=\"origin\", right_on=\"origin_icao\")\n",
    "    df = df.merge(airports_df.add_prefix(\"dest_\"), how=\"left\",\n",
    "                  left_on=\"destination\", right_on=\"dest_icao\")\n",
    "\n",
    "    # Dropping non-international flights\n",
    "    if international == False:\n",
    "        df = df.loc[df[\"origin_country\"] != df[\"dest_country\"]]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# The data is organized per month for a file. Hence, loading all files given\n",
    "# below\n",
    "flight_data = [\n",
    "    \"flightlist_20190901_20190930.csv\", \"flightlist_20191001_20191031.csv\",\n",
    "    \"flightlist_20191101_20191130.csv\", \"flightlist_20191201_20191231.csv\",\n",
    "    \"flightlist_20200101_20200131.csv\", \"flightlist_20200201_20200229.csv\",\n",
    "    \"flightlist_20200301_20200331.csv\", \"flightlist_20200401_20200430.csv\",\n",
    "    \"flightlist_20200501_20200531.csv\", \"flightlist_20200601_20200630.csv\",\n",
    "    \"flightlist_20200701_20200731.csv\", \"flightlist_20200801_20200831.csv\"\n",
    "]\n",
    "\n",
    "flights = pd.concat([load_flight_data(i, international=True) for i in tqdm(flight_data)])\n",
    "flights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded67ab5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb7d71a",
   "metadata": {},
   "source": [
    "## Making plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac257197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_us_cases():\n",
    "    sns.scatterplot(\n",
    "        x=us.loc[(us[\"date\"] < pd.to_datetime(\"2022-01\"))\n",
    "                 & (us[\"date\"] > pd.to_datetime(\"2020-03\"))][\"date\"],\n",
    "        y=us[\"cases\"], size=1, alpha=0.7\n",
    "    )\n",
    "    plt.legend(\"\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "scatter_us_cases()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107f01fa",
   "metadata": {},
   "source": [
    "### A heatmap of the prison death stratified by prison size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3442a591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_map_prison_deaths(data, radius=20, opacity=0.7):\n",
    "    fig = px.density_mapbox(data, lat=\"facility_lat\", lon=\"facility_lng\",\n",
    "                            z=\"cases_per_100\",\n",
    "                            animation_frame=\"prison_size\",\n",
    "                            animation_group=\"prison_size\",\n",
    "                            title=\"COVID-19 prison cases per 100 inmates\",\n",
    "                            mapbox_style=\"open-street-map\",\n",
    "                            hover_name=\"total_inmate_cases\",\n",
    "                            range_color=[0, 0.2],\n",
    "                            center=dict(lat=38, lon=-100), zoom=3,\n",
    "                            radius=radius, opacity=opacity)\n",
    "    fig.update_layout(margin={\"r\":0,\"t\":40,\"l\":0,\"b\":0})\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "plot_map_prison_deaths(prisons, radius=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6971a9a",
   "metadata": {},
   "source": [
    "### Making a plot of the covid cases and economic growth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4aa67c6",
   "metadata": {},
   "source": [
    "We can look at the state that was the worst hit from the pandemic in terms of the number of deaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dce529f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start, end = pd.to_datetime(\"2020-01-01\"), pd.to_datetime(\"2020-12-31\")\n",
    "states_summary = states.loc[states[\"date\"].between(start, end)] \\\n",
    "                     .groupby(\"state\") \\\n",
    "                     .mean()[[\"deaths\", \"cases\"]] \\\n",
    "                     .sort_values(by=\"deaths\", ascending=False).iloc[:5]\n",
    "\n",
    "states_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5054dde6",
   "metadata": {},
   "source": [
    "At a quick glance, it seems that New York was the worst hit by the pandemic in 2020. We can take a look at the impact on its economy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3402483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gdp_ny(data, state):\n",
    "    data = data.loc[data[\"state\"] == state]\n",
    "    \n",
    "    # Removing these summary industries from the df\n",
    "    data = data.loc[data[\"industry\"] != \"All industry total\"]\n",
    "    data = data.loc[data[\"industry\"] != \"Private industries\"]\n",
    "    \n",
    "    # Slicing based on time\n",
    "    start, end = pd.to_datetime(\"2019-01-01\"), pd.to_datetime(\"2021-01-01\")\n",
    "    data = data.loc[data[\"quarter\"].between(start, end)]\n",
    "    \n",
    "    # Making the plot\n",
    "    fig = px.line(data, x=\"quarter\", y=\"gdp\", color=\"industry\")\n",
    "        \n",
    "    fig.show()\n",
    "\n",
    "\n",
    "plot_gdp_ny(gdp_state, \"New York\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a471660a",
   "metadata": {},
   "source": [
    "## Where have the Chinese international flights gone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818a3b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_chinese_flights(data, start, end):\n",
    "    # Selecting data from september 2019 to may 2020\n",
    "    start, end = pd.to_datetime(start), pd.to_datetime(end)\n",
    "    start, end = start.tz_localize('utc'), end.tz_localize('utc')\n",
    "    df = data.loc[(data[\"day\"] > start) & (data[\"day\"] < end)]              \n",
    "    \n",
    "    # Adding a month indicator for stratification\n",
    "    df[\"month\"] = pd.DatetimeIndex(df[\"day\"]).month_name()\n",
    "    # Dropping a few nulls\n",
    "    df = df.loc[-df[\"month\"].isna()]      # 9 dropped\n",
    "    df = df.loc[-df[\"dest_type\"].isna()]  # 1 dropped\n",
    "    \n",
    "    # Selecting Chinese flights\n",
    "    df = df.loc[df[\"origin_country\"] == \"China\"]\n",
    "    \n",
    "    # Making the plot\n",
    "    title = \"Destination of Chinese international flights\"\n",
    "    fig = px.scatter_mapbox(df, lat = \"dest_latitude\", lon=\"dest_longitude\",\n",
    "                            mapbox_style=\"open-street-map\", zoom=1,\n",
    "                            center={\"lat\": 20, \"lon\": 15}, color=\"dest_type\",\n",
    "                            title=title, animation_frame=\"month\",\n",
    "                            hover_name=\"dest_name\")\n",
    "    \n",
    "    fig.update_layout(margin={\"r\":0,\"t\":30,\"l\":0,\"b\":0})\n",
    "    fig.show()\n",
    "\n",
    "    \n",
    "plot_chinese_flights(flights, start=\"2019-09-01\", end=\"2020-9-30\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b273164",
   "metadata": {},
   "source": [
    "Looking at the data from januari to march, we can see that a lot of international American airports accepted international flights from China. Notably, the following airports have received the most flights from Chinese airports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dab30ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "start, end = pd.to_datetime(\"2019-01-01\"), pd.to_datetime(\"2020-4-01\")\n",
    "start, end = start.tz_localize('utc'), end.tz_localize('utc')\n",
    "airports_analysis = flights.loc[(flights[\"day\"] > start) &\n",
    "                    (flights[\"day\"] < end) &\n",
    "                    (flights[\"dest_country\"] == \"United States\") & \n",
    "                    (flights[\"origin_country\"] == \"China\")] \\\n",
    "                    .groupby([\"dest_name\", \"dest_icao\"]).size() \\\n",
    "                    .sort_values(ascending=False)[:5]\n",
    "\n",
    "airports_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b77e730",
   "metadata": {},
   "source": [
    "We may be interested in centrality measures concerning these airports. To that end, Neo4J can be used. We can consider the airports as nodes and the flights as relationships between the airports. I first tried to use the official neo4j python driver, but soon found out that that one does not work so well. Hence, I am using py2neo, because this package was used during the lectures. We can create a graph database class and use it to encapsulate the methods that we are going to use for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49991bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the following guide for the python interface for neo4j:\n",
    "# https://py2neo.org/2021.1/index.html\n",
    "\n",
    "class FlightsGraphDatabase:\n",
    "    def __init__(self, url, user, password):\n",
    "        self.graph = Graph(url, auth=(user, password))\n",
    "        \n",
    "    def query(self, query):\n",
    "        \"\"\"Runs a Cypher query on the graph and returns the results as a\n",
    "           pandas data frame\"\"\"\n",
    "        return self.graph.run(query).to_data_frame()\n",
    "    \n",
    "    def delete_graph(self):\n",
    "        \"\"\"Deletes all nodes and relationships from the graph\"\"\"\n",
    "        self.query(\"MATCH (n) DETACH DELETE (n)\")\n",
    "        \n",
    "    def load_airports(self, airports_df):\n",
    "        \"\"\"Loads the airports in the graph as nodes. This method does not get\n",
    "           used anymore.\"\"\"\n",
    "        for index, row in airports_df.iterrows():\n",
    "            node = Node(\"Airport\", name=row[\"name\"], lat=row[\"latitude\"],\n",
    "                        lon=row[\"longitude\"], country=row[\"country\"])\n",
    "            self.graph.merge(node)\n",
    "            \n",
    "    def load_data_cypher(self, data, start=\"\", end=\"\", from_country=\"\",\n",
    "                  to_country=\"\"):\n",
    "        \"\"\"Loads airports in the database as rows and flights as relationships.\n",
    "           Both airport and flight attributes are added. This method usese a\n",
    "           cypher merge, which is slower than the load_data_bulk() method.\"\"\"\n",
    "        if start != \"\":\n",
    "            data = data.loc[data[\"day\"] > pd.to_datetime(start).tz_localize(\"utc\")]\n",
    "        if end != \"\":\n",
    "            data = data.loc[data[\"day\"] < pd.to_datetime(end).tz_localize(\"utc\")]\n",
    "        if from_country != \"\":\n",
    "            data = data.loc[data[\"origin_country\"] == from_country]\n",
    "        if to_country != \"\":\n",
    "            data = data.loc[data[\"dest_country\"] == to_country]\n",
    "            \n",
    "        print(\"Shape of data after slicing:\", data.shape)\n",
    "        \n",
    "        for index, row in data.iterrows():\n",
    "            or_airport = str(row[\"origin_icao\"])\n",
    "            or_airport_name = str(row[\"origin_name\"]).replace(\"'\", \"\")\n",
    "            or_airport_country = str(row[\"origin_country\"]).replace(\"'\", \"\")\n",
    "            dest_airport = str(row[\"dest_icao\"])\n",
    "            dest_airport_name = str(row[\"dest_name\"]).replace(\"'\", \"\")\n",
    "            dest_airport_country = str(row[\"dest_country\"]).replace(\"'\", \"\")\n",
    "            date = row[\"day\"]\n",
    "            \n",
    "            # Creating nodes and relationships\n",
    "            cypher_create = \"MERGE (n:Airport {code: '\" + or_airport +\"', name: '\" + or_airport_name + \"', country: '\" + or_airport_country + \"'})-[r:Flight {date: '\" + str(date.strftime(\"%Y-%m-%d\")) + \"'}]->(m:Airport {code: '\" + dest_airport + \"', name: '\" + dest_airport_name + \"', country: '\" + dest_airport_country + \"'})\"\n",
    "            self.query(cypher_create)\n",
    "            \n",
    "        # The airports are not connected to each other. We can use an APOC\n",
    "        # procedure to make this correction.\n",
    "        cypher_join = \"MATCH (n:Airport) WITH toLower(n.code) as code, collect(n) as nodes CALL apoc.refactor.mergeNodes(nodes) yield node RETURN *\"\n",
    "        self.query(cypher_join)\n",
    "        \n",
    "        # The previous operation has left some empty nodes behind, let's\n",
    "        # delete those now\n",
    "        cypher_delete_empty = \"MATCH (n) WHERE NOT (n)--() DELETE (n)\"\n",
    "        self.query(cypher_delete_empty)\n",
    "        \n",
    "    def load_data_bulk(self, data, from_airport_code=\"\", to_airport_code=\"\",\n",
    "                       from_country=\"\", to_country=\"\", start=\"\", end=\"\"):\n",
    "        # Making a selection of the data\n",
    "        if from_airport_code != \"\":\n",
    "            data = data.loc[data[\"origin_icao\"] == from_airport_code]\n",
    "        if to_airport_code != \"\":\n",
    "            data = data.loc[data[\"dest_icao\"] == to_airport_code]\n",
    "        if from_country != \"\":\n",
    "            data = data.loc[data[\"origin_country\"] == from_country]\n",
    "        if to_country != \"\":\n",
    "            data = data.loc[data[\"dest_country\"] == to_country]\n",
    "        if start != \"\":\n",
    "            data = data.loc[data[\"day\"] > pd.to_datetime(start).tz_localize(\"utc\")]\n",
    "        if end != \"\":\n",
    "            data = data.loc[data[\"day\"] < pd.to_datetime(end).tz_localize(\"utc\")]\n",
    "            \n",
    "        print(\"Shape of the data after slicing:\", data.shape)\n",
    "        \n",
    "        # Removing nulls\n",
    "        data = data.loc[-data[\"origin_icao\"].isna()]\n",
    "        data = data.loc[-data[\"dest_icao\"].isna()]\n",
    "        \n",
    "        print(\"Shape of the data after removing nulls:\", data.shape)\n",
    "        \n",
    "        # Doing the operation in batches or chunks\n",
    "        j = 1\n",
    "        for batch in [data[i:i+1000] for i in range(0, data.shape[0], 1000)]:\n",
    "            print(\"Batch number:\", j, \"of\", int(data.shape[0] / 1000))\n",
    "            j += 1\n",
    "            \n",
    "            # First, loading the nodes\n",
    "            name = batch.to_dict(orient=\"list\")[\"origin_name\"]\n",
    "            code = batch.to_dict(orient=\"list\")[\"origin_icao\"]\n",
    "            country = batch.to_dict(orient=\"list\")[\"origin_country\"]\n",
    "            nodes_values = [\n",
    "                [name[i], code[i], country[i]] for i in range(len(name))\n",
    "            ]\n",
    "            \n",
    "            merge_nodes(self.graph.auto(), data=nodes_values,\n",
    "                        merge_key=((\"Airport\"), \"name\", \"code\", \"country\"),\n",
    "                        keys=[\"name\", \"code\", \"country\"])\n",
    "            \n",
    "            # Second, loading the relationships\n",
    "            origin_name = batch.to_dict(orient=\"list\")[\"origin_name\"]\n",
    "            origin_code = batch.to_dict(orient=\"list\")[\"origin_icao\"]\n",
    "            origin_country = batch.to_dict(orient=\"list\")[\"origin_country\"]\n",
    "            origin_pattern = [\n",
    "                (origin_name[i], origin_code[i], origin_country[i])\n",
    "                for i in range(len(origin_country))\n",
    "            ]\n",
    "            \n",
    "            dest_name = batch.to_dict(orient=\"list\")[\"dest_name\"]\n",
    "            dest_code = batch.to_dict(orient=\"list\")[\"dest_icao\"]\n",
    "            dest_country = batch.to_dict(orient=\"list\")[\"dest_country\"]\n",
    "            dest_pattern = [\n",
    "                (dest_name[i], dest_code[i], dest_country[i])\n",
    "                for i in range(len(dest_country))\n",
    "            ]\n",
    "            \n",
    "            relationship_properties = [\n",
    "                {\"day\": i} for i in batch.to_dict(orient=\"list\")[\"day\"]\n",
    "            ]\n",
    "    \n",
    "            merge_data_relationships = [\n",
    "                [origin_pattern[i], relationship_properties[i], dest_pattern[i]]\n",
    "                for i in range(len(origin_pattern))\n",
    "                if (origin_pattern[i] != dest_pattern[i])\n",
    "            ]\n",
    "            \n",
    "            create_relationships(\n",
    "                self.graph.auto(),\n",
    "                data=merge_data_relationships,\n",
    "                rel_type=\"FLIGHT\",\n",
    "                start_node_key=(\"Airport\", \"name\", \"code\", \"country\"),\n",
    "                end_node_key=(\"Airport\", \"name\", \"code\", \"country\")\n",
    "            )\n",
    "        \n",
    "        print(\"Relationships created\")\n",
    "\n",
    "    def get_direct_flight_count(self, data=None, from_airport_code=\"\",\n",
    "                                to_airport_code=\"\", from_country=\"\",\n",
    "                                to_country=\"\", start=\"\", end=\"\"):\n",
    "        \"\"\"Gets the number of flights between two airports and/or countries\n",
    "           between 2 dates. The method uses loads the flight data that is\n",
    "           provided. In that case, it first deletes the existing graph to avoid\n",
    "           confusing results.\"\"\"\n",
    "        # Making a selection of the data\n",
    "        if from_airport_code != \"\":\n",
    "            data = data.loc[data[\"origin_icao\"] == from_airport_code]\n",
    "        if to_airport_code != \"\":\n",
    "            data = data.loc[data[\"dest_icao\"] == to_airport_code]\n",
    "        if from_country != \"\":\n",
    "            data = data.loc[data[\"origin_country\"] == from_country]\n",
    "        if to_country != \"\":\n",
    "            data = data.loc[data[\"dest_country\"] == to_country]\n",
    "        if start != \"\":\n",
    "            data = data.loc[data[\"day\"] > pd.to_datetime(start).tz_localize(\"utc\")]\n",
    "        if end != \"\":\n",
    "            data = data.loc[data[\"day\"] < pd.to_datetime(end).tz_localize(\"utc\")]\n",
    "        \n",
    "        # First, deleting the existing graph\n",
    "        self.delete_graph()\n",
    "        # Then, load the data\n",
    "        self.load_data(data)\n",
    "        # Finally, getting the information\n",
    "        cypher = \"MATCH (n)<--(m) RETURN m.name AS from_airport, COUNT(m) AS flights_to\"\n",
    "        \n",
    "        return self.query(cypher)\n",
    "\n",
    "    def get_degree(self):\n",
    "        # First, create a projection\n",
    "        self.query(\"CALL gds.graph.drop('flights', false)\")\n",
    "        self.query(\"CALL gds.graph.project('flights', 'Airport', 'FLIGHT') YIELD *\")\n",
    "        # Then, calculate the degree for each node\n",
    "        result = self.query(\"CALL gds.degree.stream('flights') YIELD nodeId, score MATCH (n:Airport) WHERE ID(n) = nodeId RETURN n.name, n.code, score\")\n",
    "        # Finally, sorting the results by degree\n",
    "        result = result.sort_values(\"score\", ascending=False)\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "# Instantiating the graph database\n",
    "bolt_url = \"bolt://localhost:7687\"\n",
    "flights_graph = FlightsGraphDatabase(bolt_url, \"neo4j\", \"root\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a51048",
   "metadata": {},
   "source": [
    "Next, we can calculate some summary statistics on the airports that we previously identified. However, these are only direct flights. The indirect flights would be much more difficult to identify, because we would also need information on the people that are in a flight. We can calculate the degree of American airports to identify which ones are the most important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23999e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_degree_weekly_us():\n",
    "    dates = [\n",
    "       #[\"2019-12-01\", \"2019-12-08\"], [\"2019-12-08\", \"2019-12-15\"],\n",
    "       #[\"2019-12-15\", \"2019-12-23\"], [\"2019-12-23\", \"2019-12-31\"],\n",
    "       #[\"2020-01-01\", \"2020-01-08\"], [\"2020-01-08\", \"2020-01-15\"],\n",
    "        [\"2020-01-15\", \"2020-01-23\"], [\"2020-01-23\", \"2020-01-31\"],\n",
    "        [\"2020-02-01\", \"2020-02-08\"], [\"2020-02-08\", \"2020-02-15\"],\n",
    "        [\"2020-02-15\", \"2020-02-21\"], [\"2020-02-21\", \"2020-02-28\"],\n",
    "        [\"2020-03-01\", \"2020-03-08\"], [\"2020-03-08\", \"2020-03-15\"],\n",
    "        [\"2020-03-15\", \"2020-03-23\"], [\"2020-03-23\", \"2020-03-31\"],\n",
    "       #[\"2020-04-01\", \"2020-04-08\"], [\"2020-04-08\", \"2020-04-15\"],\n",
    "       #[\"2020-04-15\", \"2020-04-23\"], [\"2020-04-23\", \"2020-04-30\"],\n",
    "       #[\"2020-05-01\", \"2020-05-08\"], [\"2020-05-08\", \"2020-05-15\"],\n",
    "       #[\"2020-05-15\", \"2020-05-23\"], [\"2020-05-23\", \"2020-05-31\"],\n",
    "    ]\n",
    "    \n",
    "    for i in dates:\n",
    "        file_name = (i[0] + \"_\" + i[1] + \"_degree_to_US\" + \".csv\")\n",
    "        # First, finding out if the information was already calculated, because\n",
    "        # it does take some time to load the data into neo4j\n",
    "        if file_name not in os.listdir(\"output/degree/\"):\n",
    "            print(\"Started \" + i[0])\n",
    "            # Loading the flights into neo4j\n",
    "            flights_graph.delete_graph()\n",
    "            flights_graph.load_data_bulk(flights, start=i[0], end=i[1],\n",
    "                                         to_country=\"United States\")\n",
    "            # Calculating the results\n",
    "            degree_df = flights_graph.get_degree()\n",
    "            # Saving the results as a csv file\n",
    "            degree_df.to_csv(\"output/degree/\" + i[0] + \"_\" + i[1]\n",
    "                             + \"_degree_to_US\" + \".csv\")\n",
    "            print(\"Finished \" + i[0])\n",
    "\n",
    "\n",
    "get_degree_weekly_us()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19099c46",
   "metadata": {},
   "source": [
    "We can make a plot to visualize the degree over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c84912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_degree(num_airports):\n",
    "    degree_files = os.listdir(\"output/degree/\")\n",
    "    data_degree = pd.DataFrame()\n",
    "    for file in degree_files:\n",
    "        data_individual = pd.read_csv(\"output/degree/\" + file)\n",
    "        data_individual[\"period_start\"] = file[:10]\n",
    "        data_individual = data_individual.iloc[:num_airports, 1:]\n",
    "        data_degree = pd.concat([data_degree, data_individual])\n",
    "    \n",
    "    data_degree[\"period_start\"] = pd.to_datetime(data_degree[\"period_start\"])\n",
    "    \n",
    "    return data_degree\n",
    "\n",
    "\n",
    "def plot_degree_us(num_airports=10):\n",
    "    # Loading data\n",
    "    data_degree = load_degree(num_airports=num_airports) \n",
    "    # Making the plot\n",
    "    fig = px.area(data_degree, x=\"period_start\", y=\"score\", color=\"n.name\",\n",
    "                  labels={\"score\": \"Degree\", \"n.name\": \"Airport\",\n",
    "                          \"period_start\": \"Date\"},\n",
    "                  title=\"The degree of the most popular American airports over\" \\\n",
    "                        \"time (weekly)\")\n",
    "    fig.show(\"notebook\")\n",
    "\n",
    "\n",
    "plot_degree_us(num_airports=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373051ee",
   "metadata": {},
   "source": [
    "These airports can be considered as hubs where corona could have entered the United States. Now, were these also the areas that were hit first by corona cases?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45442ec0",
   "metadata": {},
   "source": [
    "First, let's see where the top 10 most important airports are located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a692670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_airports(num_airports=10):\n",
    "    # Loading the data\n",
    "    data_degree = load_degree(num_airports=num_airports)\n",
    "    \n",
    "    # We can take the sum of the degree per airport\n",
    "    data_degree = data_degree.groupby(\"n.name\").sum()\n",
    "    \n",
    "    # Next, we need to join the latitude and longitude\n",
    "    data_degree = data_degree.merge(\n",
    "        airports_df[[\"name\", \"longitude\", \"latitude\"]], how=\"left\",\n",
    "        left_index=True, right_on=\"name\"\n",
    "    )\n",
    "    \n",
    "    # Now, making the plot\n",
    "    fig = px.scatter_mapbox(data_degree, lat=\"latitude\", lon=\"longitude\",\n",
    "                            hover_name=\"score\", mapbox_style=\"open-street-map\",\n",
    "                            zoom=2.8, size=\"score\",\n",
    "                            center={\"lat\": 38 , \"lon\": -95},\n",
    "                            title=\"Most important US airports\")\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "plot_top_airports()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dcdc45",
   "metadata": {},
   "source": [
    "Second, we can take a look where the pandemic started in the United States."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5a873b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I followed the following guide to create this plot:\n",
    "# https://plotly.com/python/choropleth-maps/\n",
    "\n",
    "def plot_daily_cases_us(df):\n",
    "    with urlopen(\"https://raw.githubusercontent.com/plotly/datasets/master/\" \\\n",
    "                 \"geojson-counties-fips.json\") as response:\n",
    "        counties_json = json.load(response)\n",
    "\n",
    "    data = df.loc[(df[\"date\"] < pd.to_datetime(\"2020-03-14\")) &\n",
    "                  (df[\"date\"] > pd.to_datetime(\"2020-02-14\"))]\n",
    "\n",
    "    fig = px.choropleth(data_frame=data,\n",
    "                        geojson=counties_json,\n",
    "                        animation_frame=\"date_str\",\n",
    "                        locations=\"fips\", color=\"cases\",\n",
    "                        color_continuous_scale=\"Blues\",\n",
    "                        scope=\"usa\",\n",
    "                      # projection=\"equirectangular\",\n",
    "                        range_color=[0, data[\"cases\"].max()])\n",
    "\n",
    "    fig.show(\"notebook\") # workaround for this plot sometimes not showing in a\n",
    "                         # notebook\n",
    "\n",
    "\n",
    "plot_daily_cases_us(counties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0311c49b",
   "metadata": {},
   "source": [
    "# Text mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1084ae85",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab415230",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cord19:\n",
    "    def __init__(self):\n",
    "        \"\"\"Loads a metadata csv that can be used to find and slice papers\n",
    "        \"\"\"\n",
    "        \n",
    "        text_metadata = pd.read_csv(\"data/cord_19/2020-12-31/metadata.csv\",\n",
    "                                    low_memory=False)\n",
    "        text_metadata[\"publish_time\"] = \\\n",
    "            pd.to_datetime(text_metadata[\"publish_time\"])\n",
    "        text_metadata = text_metadata.loc[-text_metadata[\"sha\"].isna()]\n",
    "        \n",
    "        self.metadata = text_metadata\n",
    "    \n",
    "    \n",
    "    def get_sha_list(self, start_date, end_date):\n",
    "        \"\"\"Returns a sha sum list of papers between 2 dates\n",
    "        \"\"\"\n",
    "        sha_list = text_metadata.loc[\n",
    "            (text_metadata[\"publish_time\"] > pd.to_datetime(start_date))\n",
    "            & (text_metadata[\"publish_time\"] < pd.to_datetime(end_date))\n",
    "            ][\"sha\"]\n",
    "        \n",
    "        return sha_list\n",
    "        \n",
    "        \n",
    "    def get_abstract(self, sha):\n",
    "        \"\"\"Gets the abstract of a paper based on its sha sum\n",
    "        \"\"\"\n",
    "        pdf_json_path = \"data/cord_19/2020-12-31/document_parses/pdf_json/\"\n",
    "        temp_paper = pd.read_json(pdf_json_path + str(sha) + \".json\",\n",
    "                                  orient=\"index\")\n",
    "        text_list = [i[\"text\"] for i in temp_paper.loc[\"abstract\"].values[0]]\n",
    "        \n",
    "        text = \"\"\n",
    "        for i in text_list:\n",
    "            text += str(i).lower()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    \n",
    "    def get_body(self, sha):\n",
    "        \"\"\"Gets the body text of a paper based on its sha sum\n",
    "        \"\"\"\n",
    "        pdf_json_path = \"data/cord_19/2020-12-31/document_parses/pdf_json/\"\n",
    "        temp_paper = pd.read_json(pdf_json_path + str(sha) + \".json\",\n",
    "                                  orient=\"index\")\n",
    "        text_list = [i[\"text\"] for i in temp_paper.loc[\"body_text\"].values[0]]\n",
    "    \n",
    "        text = \"\"\n",
    "        for i in text_list:\n",
    "            text += str(i).lower()\n",
    "        \n",
    "        return text \n",
    "    \n",
    "    \n",
    "    def get_text_df(self, sha_list):\n",
    "        \"\"\"Gets a data frame that contains the sha, abstract and body of papers\n",
    "           that are contained in the sha_list\"\"\"\n",
    "        abstract_list = []\n",
    "        abstract_len = []\n",
    "        body_list = []\n",
    "        body_len = []\n",
    "        \n",
    "        # Getting the abstract for every SHA and removing SHAs for which we do\n",
    "        # not have a paper\n",
    "        for sha in sha_list:\n",
    "            try:\n",
    "                abstract_list.append(self.get_abstract(sha))\n",
    "                abstract_len.append(len(self.get_abstract(sha)))\n",
    "                body_list.append(self.get_body(sha))\n",
    "                body_len.append(len(self.get_body(sha)))\n",
    "            except ValueError:\n",
    "                sha_list = sha_list.drop(sha_list.loc[sha_list == sha].index)\n",
    "        \n",
    "        # Putting the results in a dataframe\n",
    "        df = pd.DataFrame({\n",
    "            \"sha\": sha_list,\n",
    "            \"abstract\": abstract_list,\n",
    "            \"abstract_len\": abstract_len,\n",
    "            \"body\": body_list,\n",
    "            \"body_len\": body_len\n",
    "        })\n",
    "        \n",
    "        df = df.loc[df[\"abstract_len\"] != 0]\n",
    "        df = df.loc[df[\"body_len\"] > 100]\n",
    "        \n",
    "        return df\n",
    "\n",
    "    \n",
    "    def preprocess_text(self, series):\n",
    "        \"\"\"Applies some preprocessing steps to a collection of documents\n",
    "        \"\"\"\n",
    "        # Remove stopwords\n",
    "        series = [gensim.parsing.preprocessing.remove_stopwords(i)\n",
    "                  for i in series]\n",
    "        # Stem\n",
    "        series = gensim.parsing.porter.PorterStemmer().stem_documents(series)\n",
    "        # Remove numeric\n",
    "        series = [gensim.parsing.preprocessing.strip_numeric(i)\n",
    "                  for i in series]\n",
    "        # Remove punctuation\n",
    "        series = [gensim.parsing.preprocessing.strip_punctuation(i)\n",
    "                  for i in series]\n",
    "        # Remove special characters\n",
    "        series = [re.sub(\"\\\\W+\", \" \", i) for i in series]\n",
    "        # Remove short words\n",
    "        series = [gensim.parsing.preprocessing.strip_short(i) for i in series]\n",
    "        \n",
    "        return series\n",
    "\n",
    "    \n",
    "    def gen_dictionary(self, series):\n",
    "        temp = [i.split() for i in series.values.tolist()]\n",
    "        return gensim.corpora.Dictionary(temp)\n",
    "\n",
    "\n",
    "    def gen_bow(self, series):\n",
    "        \"\"\"Generates a bag of words from a series of documents\n",
    "        \"\"\"\n",
    "        temp = [i.split() for i in series.values.tolist()]\n",
    "        return [self.gen_dictionary(series).doc2bow(i) for i in temp]\n",
    "\n",
    " \n",
    "    def gen_lda_model(self, series, bow, dictionary):\n",
    "        lda_model = gensim.models.LdaMulticore(\n",
    "            corpus=bow, id2word=dictionary,\n",
    "            num_topics=10, passes=10, random_state=1337\n",
    "        )\n",
    "        \n",
    "        return lda_model\n",
    "    \n",
    "\n",
    "    def pickle_lda_model_vis(self, model, bow, dictionary, path):\n",
    "        \"\"\"Makes a visualization of an LDA model and pickles it, so that it can\n",
    "           easily be opened again without having to generate the model\n",
    "        \"\"\"\n",
    "        pyLDAvis.enable_notebook()\n",
    "        vis = pyLDAvis.gensim_models.prepare(model, bow, dictionary)\n",
    "        pickle.dump(vis, open(path, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffa9f56",
   "metadata": {},
   "source": [
    "Let's try to detect a difference between papers before and after the start of the pandemic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2e711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list = [\n",
    "    [\"2000-01\", \"2020-03\"], [\"2020-03\", \"2022\"]\n",
    "]\n",
    "\n",
    "for i in tqdm(date_list):\n",
    "    path = \"output/pickle/pyLDA_models/\" + i[0] + \"_\" + i[1] + \".p\"\n",
    "    cord_data = Cord19()\n",
    "    # Getting the body and abstract for some papers\n",
    "    sha_list = cord_data.get_sha_list(start_date=i[0],\n",
    "                                      end_date=i[1])\n",
    "    text_df = cord_data.get_text_df(sha_list)\n",
    "    \n",
    "    # Apply the preprocessing\n",
    "    text_df[\"abstract\"] = cord_data.preprocess_text(text_df[\"abstract\"])\n",
    "    text_df[\"body\"] = cord_data.preprocess_text(text_df[\"body\"])\n",
    "    \n",
    "    # Generate a dictionary and a bag of words\n",
    "    dictionary = gen_dictionary(text_df[\"body\"])\n",
    "    bow = gen_bow(text_df[\"body\"], dictionary)\n",
    "    \n",
    "    # Generate an LDA model\n",
    "    model = cord_data.gen_lda_model(text_df[\"body\"], bow, dictionary)\n",
    "    \n",
    "    # Pickle the visualisation\n",
    "    cord_data.pickle_lda_model_vis(model, bow, dictionary, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3042feaa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e28434",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.load(open(\"output/pickle/pyLDA_models/2020-01_2020-02.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1869e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.load(open(\"output/pickle/pyLDA_models/2020-02_2020-03.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9757f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.load(open(\"output/pickle/pyLDA_models/2020-03_2020-04.p\", \"rb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
