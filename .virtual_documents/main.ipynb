import sys
sys.executable


# Data storage and manipulation
import pandas as pd
import numpy as np
from py2neo import Graph, Node, Relationship
from py2neo.bulk import merge_nodes, merge_relationships, create_relationships

# Plotting
import matplotlib.pyplot as plt
import seaborn as sns
import plotly
import plotly.express as px
import plotly.figure_factory as ff
import plotly.graph_objects as go

# Extra information
from traffic.data import airports
airports_df = airports.data

# Utility
from tqdm.notebook import tqdm
from urllib.request import urlopen
import os


get_ipython().getoutput("conda info | grep 'active env'")


pd.set_option("display.max_rows", 400)
pd.set_option("display.float_format", '{:.2f}'.format)
get_ipython().run_line_magic("matplotlib", " inline")
get_ipython().run_line_magic("config", " InlineBackend.figure_formats = ['svg']")
plotly.offline.init_notebook_mode(connected=True)


# Source: https://github.com/nytimes/covid-19-data
covid_data_path = "data/covid/"

# --- Loading general covid information ---
counties = pd.read_csv(covid_data_path + "us-counties.csv")
counties = counties.rename({"cases": "cases_cum"}, axis=1)
counties["date"] = pd.to_datetime(counties["date"])
counties["date_str"] = counties["date"].dt.strftime('%Y-%m-%d')

def reverse_cumsum(series):
    series_zeroed = pd.concat([pd.Series([0]), series])
    return series_zeroed.diff()[1:]

def undo_cases_cum(df):
    counties_edited = pd.DataFrame()
    for code in df["fips"].unique():
        data = df.loc[df["fips"] == code]
        data["cases"] = reverse_cumsum(data["cases_cum"])
        counties_edited = pd.concat([counties_edited, data])
        
    return counties_edited

counties = undo_cases_cum(counties)

states = pd.read_csv(covid_data_path + "us-states.csv")
states["date"] = pd.to_datetime(states["date"])

us = pd.read_csv(covid_data_path + "us.csv")

# --- Loading information from the prisons ---
prisons = pd.read_csv(covid_data_path + "prisons/facilities.csv")
# Calculating how many cases there are per 100 inmates
prisons["cases_per_100"] = prisons["total_inmate_cases"] \
                           / (prisons["latest_inmate_population"]*100)
# Omitting outliers
prisons = prisons[prisons["cases_per_100"] < 2]

# Categorizing the prison size
prisons["prison_size"] = "unknown"
prisons.loc[prisons["latest_inmate_population"].between(0, 1000),
            "prison_size"] = "small"
prisons.loc[prisons["latest_inmate_population"].between(1000, 3000),
            "prison_size"] = "medium"
prisons.loc[prisons["latest_inmate_population"].between(3000, np.inf),
            "prison_size"] = "large"

# --- Loading information from colleges ---
colleges = pd.read_csv(covid_data_path + "colleges/colleges.csv")

# --- Loading other information ---
deaths = pd.read_csv(covid_data_path + "excess-deaths/deaths.csv")
mask_use = pd.read_csv(covid_data_path + "mask-use/mask-use-by-county.csv")


# Source: https://apps.bea.gov/regional/downloadzip.cfm
# --- Loading economic information ---
def load_gdp_data(path):
    gdp_data_path = "data/economical/SQGDP/"
    
    df = pd.read_csv(gdp_data_path + path)
    # Removing false observations
    df = df.iloc[:-4]
    # Transforming FIPS code to int
    df["GeoFIPS"] = df["GeoFIPS"].str.replace('"', "").astype(int)
    # Stripping description from left and right spaces
    df["Description"] = df["Description"].str.strip()
    # Removing invalid data
    df = df.replace("(D)", np.nan)
    
    return df


def transform_gdp_data(data):
    gdp_cols = ['2005:Q2', '2005:Q3', '2005:Q4', '2006:Q1', '2006:Q2',
                '2006:Q3', '2006:Q4', '2007:Q1', '2007:Q2', '2007:Q3',
                '2007:Q4', '2008:Q1', '2008:Q2', '2008:Q3', '2008:Q4',
                '2009:Q1', '2009:Q2', '2009:Q3', '2009:Q4', '2010:Q1',
                '2010:Q2', '2010:Q3', '2010:Q4', '2011:Q1', '2011:Q2',
                '2011:Q3', '2011:Q4', '2012:Q1', '2012:Q2', '2012:Q3',
                '2012:Q4', '2013:Q1', '2013:Q2', '2013:Q3', '2013:Q4',
                '2014:Q1', '2014:Q2', '2014:Q3', '2014:Q4', '2015:Q1',
                '2015:Q2', '2015:Q3', '2015:Q4', '2016:Q1', '2016:Q2',
                '2016:Q3', '2016:Q4', '2017:Q1', '2017:Q2', '2017:Q3',
                '2017:Q4', '2018:Q1', '2018:Q2', '2018:Q3', '2018:Q4',
                '2019:Q1', '2019:Q2', '2019:Q3', '2019:Q4', '2020:Q1',
                '2020:Q2', '2020:Q3', '2020:Q4', '2021:Q1', '2021:Q2',
                '2021:Q3', '2021:Q4']

    # It is generally not a good idea to loop over a dataframe, but I did not
    # find another way to make this transformation
    gdp_df = pd.DataFrame(index=gdp_cols)
    for state in data["GeoName"].unique():
        data_state = data.loc[data["GeoName"] == state]
        for var in data_state["Description"]:
            gdp = data_state.loc[data["Description"] == var][gdp_cols]\
                .transpose()
            # 1d dataframe to series
            gdp = gdp.iloc[:, 0]
            # Renaming the series
            gdp = gdp.rename(state + "|" + var)
            # Adding to the data frame
            gdp_df = gdp_df.join(gdp)

    # Making the table long instead of wide
    gdp_df = pd.melt(gdp_df, value_vars=gdp_df.columns.values,
                     var_name="industry", value_name="gdp", ignore_index=False)

    # Splitting industry and state
    gdp_df[["state", "industry"]] = gdp_df["industry"].str\
        .split("|", 1, expand=True)

    # Including the index (quarter) as a variable
    gdp_df["quarter"] = gdp_df.index
    gdp_df["quarter"] = pd.to_datetime(gdp_df["quarter"].str.replace(":", "-"))
    
    # Resetting the index
    gdp_df = gdp_df.reset_index()
    gdp_df = gdp_df.drop("index", axis=1)
    
    # Changing column types
    gdp_df["gdp"] = pd.to_numeric(gdp_df["gdp"])

    return gdp_df



gdp_state = load_gdp_data("SQGDP9__ALL_AREAS_2005_2021.csv")
gdp_state = transform_gdp_data(gdp_state)


# Source: https://zenodo.org/record/6411336#.YmO_gS8RrzU
def load_flight_data(path, international=False):
    """Loads flight data. To save on memory, it is possible to discard non-
       international flights. Merges information about airports."""
    flight_data_path = "data/flights_open_sky/"
    df = pd.read_csv(flight_data_path + path)

    # Dropping where important variables are null. We cannot do much with the
    # data if the origin or destination of the flight are not known.
    df = df.loc[-df["origin"].isnull()]
    df = df.loc[-df["destination"].isnull()]

    # Type changes
    df["firstseen"] =  pd.to_datetime(df["firstseen"])
    df["lastseen"] =  pd.to_datetime(df["lastseen"])
    df["day"] =  pd.to_datetime(df["day"])

    # Adding information about the origin and destination airport
    df = df.merge(airports_df.add_prefix("origin_"), how="left",
                  left_on="origin", right_on="origin_icao")
    df = df.merge(airports_df.add_prefix("dest_"), how="left",
                  left_on="destination", right_on="dest_icao")

    # Dropping non-international flights
    if international == False:
        df = df.loc[df["origin_country"] != df["dest_country"]]

    return df


# The data is organized per month for a file. Hence, loading all files given
# below
flight_data = [
    "flightlist_20190901_20190930.csv", "flightlist_20191001_20191031.csv",
    "flightlist_20191101_20191130.csv", "flightlist_20191201_20191231.csv",
    "flightlist_20200101_20200131.csv", "flightlist_20200201_20200229.csv",
    "flightlist_20200301_20200331.csv", "flightlist_20200401_20200430.csv",
    "flightlist_20200501_20200531.csv", "flightlist_20200601_20200630.csv",
    "flightlist_20200701_20200731.csv", "flightlist_20200801_20200831.csv"
]

flights = pd.concat([load_flight_data(i, international=True) for i in tqdm(flight_data)])
flights.shape


def plot_map_prison_deaths(data, radius=20, opacity=0.7):
    fig = px.density_mapbox(data, lat="facility_lat", lon="facility_lng",
                            z="cases_per_100",
                            animation_frame="prison_size",
                            animation_group="prison_size",
                            title="COVID-19 prison cases per 100 inmates",
                            mapbox_style="open-street-map",
                            hover_name="total_inmate_cases",
                            range_color=[0, 0.2],
                            center=dict(lat=38, lon=-100), zoom=3,
                            radius=radius, opacity=opacity)
    fig.update_layout(margin={"r":0,"t":40,"l":0,"b":0})
    fig.show()


plot_map_prison_deaths(prisons, radius=30)


start, end = pd.to_datetime("2020-01-01"), pd.to_datetime("2020-12-31")
states_summary = states.loc[states["date"].between(start, end)] \
                     .groupby("state") \
                     .mean()[["deaths", "cases"]] \
                     .sort_values(by="deaths", ascending=False).iloc[:5]

states_summary


def plot_gdp_ny(data, state):
    data = data.loc[data["state"] == state]
    
    # Removing these summary industries from the df
    data = data.loc[data["industry"] != "All industry total"]
    data = data.loc[data["industry"] != "Private industries"]
    
    # Slicing based on time
    start, end = pd.to_datetime("2019-01-01"), pd.to_datetime("2021-01-01")
    data = data.loc[data["quarter"].between(start, end)]
    
    # Making the plot
    fig = px.line(data, x="quarter", y="gdp", color="industry")
        
    fig.show()


plot_gdp_ny(gdp_state, "New York")


def plot_chinese_flights(data, start, end):
    # Selecting data from september 2019 to may 2020
    start, end = pd.to_datetime(start), pd.to_datetime(end)
    start, end = start.tz_localize('utc'), end.tz_localize('utc')
    df = data.loc[(data["day"] > start) & (data["day"] < end)]              
    
    # Adding a month indicator for stratification
    df["month"] = pd.DatetimeIndex(df["day"]).month_name()
    # Dropping a few nulls
    df = df.loc[-df["month"].isna()]      # 9 dropped
    df = df.loc[-df["dest_type"].isna()]  # 1 dropped
    
    # Selecting Chinese flights
    df = df.loc[df["origin_country"] == "China"]
    
    # Making the plot
    title = "Destination of Chinese international flights"
    fig = px.scatter_mapbox(df, lat = "dest_latitude", lon="dest_longitude",
                            mapbox_style="open-street-map", zoom=1,
                            center={"lat": 20, "lon": 15}, color="dest_type",
                            title=title, animation_frame="month",
                            hover_name="dest_name")
    
    fig.update_layout(margin={"r":0,"t":30,"l":0,"b":0})
    fig.show()

    
plot_chinese_flights(flights, start="2019-09-01", end="2020-9-30")


start, end = pd.to_datetime("2019-01-01"), pd.to_datetime("2020-4-01")
start, end = start.tz_localize('utc'), end.tz_localize('utc')
airports_analysis = flights.loc[(flights["day"] > start) &
                    (flights["day"] < end) &
                    (flights["dest_country"] == "United States") & 
                    (flights["origin_country"] == "China")] \
                    .groupby(["dest_name", "dest_icao"]).size() \
                    .sort_values(ascending=False)[:5]

airports_analysis


# Using the following guide for the python interface for neo4j:
# https://py2neo.org/2021.1/index.html

class FlightsGraphDatabase:
    def __init__(self, url, user, password):
        self.graph = Graph(url, auth=(user, password))
        
    def query(self, query):
        """Runs a Cypher query on the graph and returns the results as a
           pandas data frame"""
        return self.graph.run(query).to_data_frame()
    
    def delete_graph(self):
        """Deletes all nodes and relationships from the graph"""
        self.query("MATCH (n) DETACH DELETE (n)")
        
    def load_airports(self, airports_df):
        """Loads the airports in the graph as nodes. This method does not get
           used anymore."""
        for index, row in airports_df.iterrows():
            node = Node("Airport", name=row["name"], lat=row["latitude"],
                        lon=row["longitude"], country=row["country"])
            self.graph.merge(node)
            
    def load_data_cypher(self, data, start="", end="", from_country="",
                  to_country=""):
        """Loads airports in the database as rows and flights as relationships.
           Both airport and flight attributes are added. This method usese a
           cypher merge, which is slower than the load_data_bulk() method."""
        if start != "":
            data = data.loc[data["day"] > pd.to_datetime(start).tz_localize("utc")]
        if end != "":
            data = data.loc[data["day"] < pd.to_datetime(end).tz_localize("utc")]
        if from_country != "":
            data = data.loc[data["origin_country"] == from_country]
        if to_country != "":
            data = data.loc[data["dest_country"] == to_country]
            
        print("Shape of data after slicing:", data.shape)
        
        for index, row in data.iterrows():
            or_airport = str(row["origin_icao"])
            or_airport_name = str(row["origin_name"]).replace("'", "")
            or_airport_country = str(row["origin_country"]).replace("'", "")
            dest_airport = str(row["dest_icao"])
            dest_airport_name = str(row["dest_name"]).replace("'", "")
            dest_airport_country = str(row["dest_country"]).replace("'", "")
            date = row["day"]
            
            # Creating nodes and relationships
            cypher_create = "MERGE (n:Airport {code: '" + or_airport +"', name: '" + or_airport_name + "', country: '" + or_airport_country + "'})-[r:Flight {date: '" + str(date.strftime("%Y-%m-%d")) + "'}]->(m:Airport {code: '" + dest_airport + "', name: '" + dest_airport_name + "', country: '" + dest_airport_country + "'})"
            self.query(cypher_create)
            
        # The airports are not connected to each other. We can use an APOC
        # procedure to make this correction.
        cypher_join = "MATCH (n:Airport) WITH toLower(n.code) as code, collect(n) as nodes CALL apoc.refactor.mergeNodes(nodes) yield node RETURN *"
        self.query(cypher_join)
        
        # The previous operation has left some empty nodes behind, let's
        # delete those now
        cypher_delete_empty = "MATCH (n) WHERE NOT (n)--() DELETE (n)"
        self.query(cypher_delete_empty)
        
    def load_data_bulk(self, data, from_airport_code="", to_airport_code="",
                       from_country="", to_country="", start="", end=""):
        # Making a selection of the data
        if from_airport_code != "":
            data = data.loc[data["origin_icao"] == from_airport_code]
        if to_airport_code != "":
            data = data.loc[data["dest_icao"] == to_airport_code]
        if from_country != "":
            data = data.loc[data["origin_country"] == from_country]
        if to_country != "":
            data = data.loc[data["dest_country"] == to_country]
        if start != "":
            data = data.loc[data["day"] > pd.to_datetime(start).tz_localize("utc")]
        if end != "":
            data = data.loc[data["day"] < pd.to_datetime(end).tz_localize("utc")]
            
        print("Shape of the data after slicing:", data.shape)
        
        # Removing nulls
        data = data.loc[-data["origin_icao"].isna()]
        data = data.loc[-data["dest_icao"].isna()]
        
        print("Shape of the data after removing nulls:", data.shape)
        
        # Doing the operation in batches or chunks
        j = 1
        for batch in [data[i:i+1000] for i in range(0, data.shape[0], 1000)]:
            print("Batch number:", j, "of", int(data.shape[0] / 1000))
            j += 1
            
            # First, loading the nodes
            name = batch.to_dict(orient="list")["origin_name"]
            code = batch.to_dict(orient="list")["origin_icao"]
            country = batch.to_dict(orient="list")["origin_country"]
            nodes_values = [
                [name[i], code[i], country[i]] for i in range(len(name))
            ]
            
            merge_nodes(self.graph.auto(), data=nodes_values,
                        merge_key=(("Airport"), "name", "code", "country"),
                        keys=["name", "code", "country"])
            
            # Second, loading the relationships
            origin_name = batch.to_dict(orient="list")["origin_name"]
            origin_code = batch.to_dict(orient="list")["origin_icao"]
            origin_country = batch.to_dict(orient="list")["origin_country"]
            origin_pattern = [
                (origin_name[i], origin_code[i], origin_country[i])
                for i in range(len(origin_country))
            ]
            
            dest_name = batch.to_dict(orient="list")["dest_name"]
            dest_code = batch.to_dict(orient="list")["dest_icao"]
            dest_country = batch.to_dict(orient="list")["dest_country"]
            dest_pattern = [
                (dest_name[i], dest_code[i], dest_country[i])
                for i in range(len(dest_country))
            ]
            
            relationship_properties = [
                {"day": i} for i in batch.to_dict(orient="list")["day"]
            ]
    
            merge_data_relationships = [
                [origin_pattern[i], relationship_properties[i], dest_pattern[i]]
                for i in range(len(origin_pattern))
                if (origin_pattern[i] != dest_pattern[i])
            ]
            
            create_relationships(
                self.graph.auto(),
                data=merge_data_relationships,
                rel_type="FLIGHT",
                start_node_key=("Airport", "name", "code", "country"),
                end_node_key=("Airport", "name", "code", "country")
            )
        
        print("Relationships created")

    def get_direct_flight_count(self, data=None, from_airport_code="",
                                to_airport_code="", from_country="",
                                to_country="", start="", end=""):
        """Gets the number of flights between two airports and/or countries
           between 2 dates. The method uses loads the flight data that is
           provided. In that case, it first deletes the existing graph to avoid
           confusing results."""
        # Making a selection of the data
        if from_airport_code != "":
            data = data.loc[data["origin_icao"] == from_airport_code]
        if to_airport_code != "":
            data = data.loc[data["dest_icao"] == to_airport_code]
        if from_country != "":
            data = data.loc[data["origin_country"] == from_country]
        if to_country != "":
            data = data.loc[data["dest_country"] == to_country]
        if start != "":
            data = data.loc[data["day"] > pd.to_datetime(start).tz_localize("utc")]
        if end != "":
            data = data.loc[data["day"] < pd.to_datetime(end).tz_localize("utc")]
        
        # First, deleting the existing graph
        self.delete_graph()
        # Then, load the data
        self.load_data(data)
        # Finally, getting the information
        cypher = "MATCH (n)<--(m) RETURN m.name AS from_airport, COUNT(m) AS flights_to"
        
        return self.query(cypher)

    def get_degree(self):
        # First, create a projection
        self.query("CALL gds.graph.drop('flights', false)")
        self.query("CALL gds.graph.project('flights', 'Airport', 'FLIGHT') YIELD *")
        # Then, calculate the degree for each node
        result = self.query("CALL gds.degree.stream('flights') YIELD nodeId, score MATCH (n:Airport) WHERE ID(n) = nodeId RETURN n.name, n.code, score")
        # Finally, sorting the results by degree
        result = result.sort_values("score", ascending=False)
        
        return result


# Instantiating the graph database
bolt_url = "bolt://localhost:7687"
flights_graph = FlightsGraphDatabase(bolt_url, "neo4j", "root")


def get_degree_weekly_us():
    dates = [
       #["2019-12-01", "2019-12-08"], ["2019-12-08", "2019-12-15"],
       #["2019-12-15", "2019-12-23"], ["2019-12-23", "2019-12-31"],
       #["2020-01-01", "2020-01-08"], ["2020-01-08", "2020-01-15"],
        ["2020-01-15", "2020-01-23"], ["2020-01-23", "2020-01-31"],
        ["2020-02-01", "2020-02-08"], ["2020-02-08", "2020-02-15"],
        ["2020-02-15", "2020-02-21"], ["2020-02-21", "2020-02-28"],
        ["2020-03-01", "2020-03-08"], ["2020-03-08", "2020-03-15"],
        ["2020-03-15", "2020-03-23"], ["2020-03-23", "2020-03-31"],
       #["2020-04-01", "2020-04-08"], ["2020-04-08", "2020-04-15"],
       #["2020-04-15", "2020-04-23"], ["2020-04-23", "2020-04-30"],
       #["2020-05-01", "2020-05-08"], ["2020-05-08", "2020-05-15"],
       #["2020-05-15", "2020-05-23"], ["2020-05-23", "2020-05-31"],
    ]
    
    for i in dates:
        file_name = (i[0] + "_" + i[1] + "_degree_to_US" + ".csv")
        # First, finding out if the information was already calculated, because
        # it does take some time to load the data into neo4j
        if file_name not in os.listdir("output/degree/"):
            print("Started " + i[0])
            # Loading the flights into neo4j
            flights_graph.delete_graph()
            flights_graph.load_data_bulk(flights, start=i[0], end=i[1],
                                         to_country="United States")
            # Calculating the results
            degree_df = flights_graph.get_degree()
            # Saving the results as a csv file
            degree_df.to_csv("output/degree/" + i[0] + "_" + i[1]
                             + "_degree_to_US" + ".csv")
            print("Finished " + i[0])


get_degree_weekly_us()


def plot_degree_us(num_airports=10):
    # Loading data
    degree_files = os.listdir("output/degree/")
    data_degree = pd.DataFrame()
    for file in degree_files:
        data_individual = pd.read_csv("output/degree/" + file)
        data_individual["period_start"] = file[:10]
        data_individual = data_individual.iloc[:num_airports, 1:]
        data_degree = pd.concat([data_degree, data_individual])
        
    # Making the plot
    fig = px.area(data_degree, x="period_start", y="score", color="n.name",
                  labels={"score": "Degree", "n.name": "Airport",
                          "period_start": "Date"},
                  title="The degree of the most popular American airports over" \
                        "time (weekly)")
    fig.show()


plot_degree_us(num_airports=20)


# I followed the following guide to create this plot:
# https://plotly.com/python/choropleth-maps/

def plot_daily_cases_us(df):
    with urlopen('https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json') as response:
        counties_json = json.load(response)
    
    data = df.loc[df["date"] < pd.to_datetime("2020-03-01")]
    
    fig = px.choropleth(data_frame=data,
                        geojson=counties_json,
                        animation_frame="date_str",
                        hover_name="cases",
                        locations="fips", color="cases",
                        color_continuous_scale="Blues",
                        scope="usa", projection="equirectangular",
                        range_color=[0, data["cases"].max()]
                        )
    
    fig.show("notebook") # workaround for plots sometimes not showing in a
                         # notebook


plot_daily_cases_us(counties)
